# inference_engine.py - ì¶”ë¡  ì—”ì§„
"""
YOLOv8 í™”ì¬ ë° ì—°ê¸° ê°ì§€ ì¶”ë¡  ì—”ì§„ ëª¨ë“ˆ
"""

import os
import cv2
import time
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import torch
from ultralytics import YOLO
from config import Config

class InferenceEngine:
    """ì¶”ë¡  ì—”ì§„ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path=None, confidence=0.5, iou_threshold=0.45):
        self.model_path = model_path
        self.model = None
        self.confidence = confidence
        self.iou_threshold = iou_threshold
        self.class_names = Config.CLASS_NAMES
        self.colors = self._generate_colors()
        
    def _generate_colors(self):
        """í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ ìƒì„±"""
        colors = {}
        color_palette = [
            (255, 0, 0),    # ë¹¨ê°„ìƒ‰ - Fire
            (0, 255, 0),    # ì´ˆë¡ìƒ‰ - default
            (128, 128, 128) # íšŒìƒ‰ - smoke
        ]
        
        for i, class_name in enumerate(self.class_names):
            if i < len(color_palette):
                colors[class_name] = color_palette[i]
            else:
                # ëœë¤ ìƒ‰ìƒ ìƒì„±
                colors[class_name] = tuple(np.random.randint(0, 255, 3).tolist())
        
        return colors
    
    def load_model(self, model_path=None):
        """ëª¨ë¸ ë¡œë“œ"""
        if model_path:
            self.model_path = model_path
        
        if not self.model_path or not os.path.exists(self.model_path):
            # ê¸°ë³¸ ê²½ë¡œì—ì„œ ëª¨ë¸ ì°¾ê¸°
            default_paths = [
                os.path.join(Config.HOME, 'runs', 'detect', 'fire_smoke_detection', 'weights', 'best.pt'),
                os.path.join(Config.HOME, 'runs', 'detect', 'train', 'weights', 'best.pt'),
                'yolov8n.pt'  # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ í´ë°±
            ]
            
            for path in default_paths:
                if os.path.exists(path):
                    self.model_path = path
                    break
            else:
                print("âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
        
        try:
            self.model = YOLO(self.model_path)
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
            
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ GPUë¡œ ì´ë™
            if torch.cuda.is_available():
                self.model.to('cuda')
                print("ğŸš€ GPU ê°€ì† ì‚¬ìš©")
            
            return True
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def predict_image(self, image_path, save_result=True, show_result=True):
        """ë‹¨ì¼ ì´ë¯¸ì§€ ì¶”ë¡ """
        if self.model is None:
            print("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        if not os.path.exists(image_path):
            print(f"âŒ ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
            return None
        
        print(f"=== ì´ë¯¸ì§€ ì¶”ë¡ : {os.path.basename(image_path)} ===")
        
        try:
            # ì¶”ë¡  ì‹¤í–‰
            results = self.model.predict(
                source=image_path,
                conf=self.confidence,
                iou=self.iou_threshold,
                save=save_result,
                show_labels=True,
                show_conf=True,
                line_width=2
            )
            
            # ê²°ê³¼ ë¶„ì„
            result = results[0]
            detections = self._parse_detections(result)
            
            # ê²°ê³¼ ì¶œë ¥
            self._print_detection_results(detections)
            
            # ì‹œê°í™”
            if show_result:
                self._visualize_result(result, image_path)
            
            return {
                'detections': detections,
                'raw_results': results,
                'image_path': image_path
            }
            
        except Exception as e:
            print(f"âŒ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None
    
    def predict_batch(self, image_folder, output_folder=None):
        """ë°°ì¹˜ ì´ë¯¸ì§€ ì¶”ë¡ """
        if self.model is None:
            print("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"=== ë°°ì¹˜ ì¶”ë¡ : {image_folder} ===")
        
        # ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
        image_files = []
        
        for ext in image_extensions:
            image_files.extend([
                os.path.join(image_folder, f) for f in os.listdir(image_folder)
                if f.lower().endswith(ext.lower())
            ])
        
        if not image_files:
            print("âŒ ì²˜ë¦¬í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"ì´ {len(image_files)}ê°œ ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜ˆì •")
        
        try:
            # ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰
            results = self.model.predict(
                source=image_folder,
                conf=self.confidence,
                iou=self.iou_threshold,
                save=True,
                project=output_folder or 'batch_inference',
                exist_ok=True
            )
            
            # ê²°ê³¼ ë¶„ì„
            batch_results = []
            for i, result in enumerate(results):
                detections = self._parse_detections(result)
                batch_results.append({
                    'image_path': image_files[i] if i < len(image_files) else f"image_{i}",
                    'detections': detections
                })
            
            # ë°°ì¹˜ ê²°ê³¼ ìš”ì•½
            self._print_batch_summary(batch_results)
            
            return batch_results
            
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None
    
    def predict_video(self, video_path, output_path=None, show_live=False):
        """ë¹„ë””ì˜¤ ì¶”ë¡ """
        if self.model is None:
            print("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"=== ë¹„ë””ì˜¤ ì¶”ë¡ : {os.path.basename(video_path)} ===")
        
        try:
            cap = cv2.VideoCapture(video_path)
            
            if not cap.isOpened():
                print("âŒ ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ë¹„ë””ì˜¤ ì •ë³´
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            print(f"ë¹„ë””ì˜¤ ì •ë³´: {width}x{height}, {fps}FPS, {total_frames}í”„ë ˆì„")
            
            # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
            if output_path:
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            
            frame_count = 0
            detection_history = []
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                
                # ì¶”ë¡  ì‹¤í–‰
                results = self.model.predict(
                    source=frame,
                    conf=self.confidence,
                    iou=self.iou_threshold,
                    verbose=False
                )
                
                # ê²°ê³¼ ê·¸ë¦¬ê¸°
                annotated_frame = results[0].plot()
                
                # ê°ì§€ ê²°ê³¼ ì €ì¥
                detections = self._parse_detections(results[0])
                if detections:
                    detection_history.append({
                        'frame': frame_count,
                        'time': frame_count / fps,
                        'detections': detections
                    })
                
                # ì¶œë ¥ ë¹„ë””ì˜¤ì— ì €ì¥
                if output_path:
                    out.write(annotated_frame)
                
                # ì‹¤ì‹œê°„ í‘œì‹œ
                if show_live:
                    cv2.imshow('Fire & Smoke Detection', annotated_frame)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if frame_count % 30 == 0:
                    progress = (frame_count / total_frames) * 100
                    print(f"ì§„í–‰ë¥ : {progress:.1f}% ({frame_count}/{total_frames})")
            
            # ì •ë¦¬
            cap.release()
            if output_path:
                out.release()
            cv2.destroyAllWindows()
            
            print(f"âœ… ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(detection_history)}ê°œ í”„ë ˆì„ì—ì„œ ê°ì§€")
            
            return {
                'total_frames': total_frames,
                'processed_frames': frame_count,
                'detection_history': detection_history,
                'output_path': output_path
            }
            
        except Exception as e:
            print(f"âŒ ë¹„ë””ì˜¤ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            return None
    
    def real_time_detection(self, camera_index=0, save_video=False, output_path='real_time_detection.mp4'):
        """ì‹¤ì‹œê°„ ì›¹ìº  ê°ì§€"""
        if self.model is None:
            print("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        print("=== ì‹¤ì‹œê°„ í™”ì¬/ì—°ê¸° ê°ì§€ ì‹œì‘ ===")
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'q' í‚¤ë¥¼ ëˆ„ë¥´ì„¸ìš”.")
        
        try:
            cap = cv2.VideoCapture(camera_index)
            
            if not cap.isOpened():
                print("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ì¹´ë©”ë¼ ì„¤ì •
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            cap.set(cv2.CAP_PROP_FPS, 30)
            
            # ë¹„ë””ì˜¤ ì €ì¥ ì„¤ì •
            if save_video:
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                out = cv2.VideoWriter(output_path, fourcc, 20.0, (640, 480))
            
            detection_count = 0
            fps_counter = 0
            start_time = time.time()
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    print("í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                fps_counter += 1
                
                # ì¶”ë¡  ì‹¤í–‰
                results = self.model.predict(
                    source=frame,
                    conf=self.confidence,
                    iou=self.iou_threshold,
                    verbose=False
                )
                
                # ê²°ê³¼ ê·¸ë¦¬ê¸°
                annotated_frame = results[0].plot()
                
                # FPS ê³„ì‚° ë° í‘œì‹œ
                elapsed_time = time.time() - start_time
                if elapsed_time > 0:
                    fps = fps_counter / elapsed_time
                    cv2.putText(annotated_frame, f'FPS: {fps:.1f}', 
                               (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                
                # ê°ì§€ ìˆ˜ í‘œì‹œ
                detections = self._parse_detections(results[0])
                if detections:
                    detection_count += len(detections)
                    
                    # ì•Œë¦¼ ë©”ì‹œì§€
                    for detection in detections:
                        class_name = detection['class']
                        if class_name in ['Fire', 'smoke']:
                            cv2.putText(annotated_frame, f'ALERT: {class_name} DETECTED!', 
                                       (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)
                
                # í™”ë©´ í‘œì‹œ
                cv2.imshow('Fire & Smoke Detection (Press Q to quit)', annotated_frame)
                
                # ë¹„ë””ì˜¤ ì €ì¥
                if save_video:
                    out.write(annotated_frame)
                
                # 'q' í‚¤ë¡œ ì¢…ë£Œ
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
            
            # ì •ë¦¬
            cap.release()
            if save_video:
                out.release()
            cv2.destroyAllWindows()
            
            print(f"âœ… ì‹¤ì‹œê°„ ê°ì§€ ì¢…ë£Œ. ì´ {detection_count}ê°œ ê°ì²´ ê°ì§€")
            
            return {
                'total_detections': detection_count,
                'duration': elapsed_time,
                'fps': fps_counter / elapsed_time if elapsed_time > 0 else 0
            }
            
        except Exception as e:
            print(f"âŒ ì‹¤ì‹œê°„ ê°ì§€ ì‹¤íŒ¨: {e}")
            return None
    
    def _parse_detections(self, result):
        """ê°ì§€ ê²°ê³¼ íŒŒì‹±"""
        detections = []
        
        if result.boxes is not None and len(result.boxes) > 0:
            boxes = result.boxes.xyxy.cpu().numpy()  # x1, y1, x2, y2
            confidences = result.boxes.conf.cpu().numpy()
            classes = result.boxes.cls.cpu().numpy()
            
            for i in range(len(boxes)):
                class_id = int(classes[i])
                if class_id < len(self.class_names):
                    detection = {
                        'class': self.class_names[class_id],
                        'class_id': class_id,
                        'confidence': float(confidences[i]),
                        'bbox': boxes[i].tolist(),  # [x1, y1, x2, y2]
                        'center': [
                            (boxes[i][0] + boxes[i][2]) / 2,
                            (boxes[i][1] + boxes[i][3]) / 2
                        ]
                    }
                    detections.append(detection)
        
        return detections
    
    def _print_detection_results(self, detections):
        """ê°ì§€ ê²°ê³¼ ì¶œë ¥"""
        if not detections:
            print("âŒ ê°ì§€ëœ ê°ì²´ ì—†ìŒ")
            return
        
        print(f"âœ… {len(detections)}ê°œ ê°ì²´ ê°ì§€:")
        
        class_counts = {}
        for detection in detections:
            class_name = detection['class']
            confidence = detection['confidence']
            
            if class_name not in class_counts:
                class_counts[class_name] = []
            class_counts[class_name].append(confidence)
            
            print(f"  - {class_name}: {confidence:.3f}")
        
        # í´ë˜ìŠ¤ë³„ ìš”ì•½
        print("\nğŸ“Š í´ë˜ìŠ¤ë³„ ìš”ì•½:")
        for class_name, confidences in class_counts.items():
            avg_conf = np.mean(confidences)
            max_conf = np.max(confidences)
            print(f"  {class_name}: {len(confidences)}ê°œ (í‰ê· : {avg_conf:.3f}, ìµœëŒ€: {max_conf:.3f})")
    
    def _print_batch_summary(self, batch_results):
        """ë°°ì¹˜ ê²°ê³¼ ìš”ì•½"""
        total_images = len(batch_results)
        images_with_detections = sum(1 for r in batch_results if r['detections'])
        total_detections = sum(len(r['detections']) for r in batch_results)
        
        print(f"\n=== ë°°ì¹˜ ì²˜ë¦¬ ìš”ì•½ ===")
        print(f"ì´ ì´ë¯¸ì§€: {total_images}ê°œ")
        print(f"ê°ì§€ëœ ì´ë¯¸ì§€: {images_with_detections}ê°œ")
        print(f"ì´ ê°ì§€ ê°ì²´: {total_detections}ê°œ")
        print(f"ê°ì§€ìœ¨: {images_with_detections/total_images*100:.1f}%")
        
        # í´ë˜ìŠ¤ë³„ í†µê³„
        class_stats = {}
        for result in batch_results:
            for detection in result['detections']:
                class_name = detection['class']
                if class_name not in class_stats:
                    class_stats[class_name] = []
                class_stats[class_name].append(detection['confidence'])
        
        if class_stats:
            print(f"\nğŸ“Š í´ë˜ìŠ¤ë³„ í†µê³„:")
            for class_name, confidences in class_stats.items():
                count = len(confidences)
                avg_conf = np.mean(confidences)
                print(f"  {class_name}: {count}ê°œ (í‰ê·  ì‹ ë¢°ë„: {avg_conf:.3f})")
    
    def _visualize_result(self, result, image_path):
        """ê²°ê³¼ ì‹œê°í™”"""
        try:
            # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ
            image = Image.open(image_path)
            
            # ê²°ê³¼ ì´ë¯¸ì§€ ìƒì„±
            result_image = result.plot()
            result_image_pil = Image.fromarray(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))
            
            # ì‹œê°í™”
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))
            
            # ì›ë³¸ ì´ë¯¸ì§€
            ax1.imshow(image)
            ax1.set_title('Original Image')
            ax1.axis('off')
            
            # ê²°ê³¼ ì´ë¯¸ì§€
            ax2.imshow(result_image_pil)
            ax2.set_title('Detection Results')
            ax2.axis('off')
            
            plt.tight_layout()
            plt.show()
            
        except Exception as e:
            print(f"ì‹œê°í™” ì‹¤íŒ¨: {e}")
    
    def set_confidence_threshold(self, confidence):
        """ì‹ ë¢°ë„ ì„ê³„ê°’ ì„¤ì •"""
        self.confidence = confidence
        print(f"ì‹ ë¢°ë„ ì„ê³„ê°’ ë³€ê²½: {confidence}")
    
    def set_iou_threshold(self, iou_threshold):
        """IoU ì„ê³„ê°’ ì„¤ì •"""
        self.iou_threshold = iou_threshold
        print(f"IoU ì„ê³„ê°’ ë³€ê²½: {iou_threshold}")
    
    def get_model_info(self):
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        if self.model is None:
            print("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        info = {
            'model_path': self.model_path,
            'confidence_threshold': self.confidence,
            'iou_threshold': self.iou_threshold,
            'class_names': self.class_names,
            'device': next(self.model.model.parameters()).device if self.model else 'unknown'
        }
        
        print("=== ëª¨ë¸ ì •ë³´ ===")
        for key, value in info.items():
            print(f"{key}: {value}")
        
        return info

def main():
    """ì¶”ë¡  ì—”ì§„ í…ŒìŠ¤íŠ¸"""
    # ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”
    engine = InferenceEngine(confidence=0.5)
    
    # ëª¨ë¸ ë¡œë“œ
    if engine.load_model():
        print("ì¶”ë¡  ì—”ì§„ ì¤€ë¹„ ì™„ë£Œ!")
        print("\nì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:")
        print("- engine.predict_image('ì´ë¯¸ì§€ê²½ë¡œ'): ë‹¨ì¼ ì´ë¯¸ì§€ ì¶”ë¡ ")
        print("- engine.predict_batch('í´ë”ê²½ë¡œ'): ë°°ì¹˜ ì¶”ë¡ ")
        print("- engine.predict_video('ë¹„ë””ì˜¤ê²½ë¡œ'): ë¹„ë””ì˜¤ ì¶”ë¡ ")
        print("- engine.real_time_detection(): ì‹¤ì‹œê°„ ê°ì§€")
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        engine.get_model_info()

if __name__ == "__main__":
    main()